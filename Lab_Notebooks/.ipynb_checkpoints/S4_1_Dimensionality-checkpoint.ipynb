{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/S4_1_Dimensionality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fab2zKXwAinB"
   },
   "source": [
    "##**Chapter 8 – Dimensionality Reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7Q5WigQxsVd"
   },
   "source": [
    "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EX7KlNGWYypLnH_53OnJR6oBjfgb_gCZ4gmnOeR68a6zMA?download=1'>\n",
    "<center> Caption: <i>Denise diagnoses an overheated CPU at our data center in The Dalles, Oregon. <br> For more than a decade, we have built some of the world's most efficient servers.</i> <br> Photo from the <a href='https://www.google.com/about/datacenters/gallery/'>Google Data Center gallery</a> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGGHmOj1ygXe"
   },
   "source": [
    "*Our world is increasingly filled with data from all sorts of sources, including environmental data. Can we reduce the data to a reduced, meaningful space to save on computation time and increase explainability?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlTDG-57-aAI"
   },
   "source": [
    "This notebook will be used in the lab session for week 4 of the course, covers Chapters 8 of Géron, and builds on the [notebooks made available on _Github_](https://github.com/ageron/handson-ml2).\n",
    "\n",
    "Need a reminder of last week's labs? Click [_here_](https://colab.research.google.com/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/Week_3_Decision_Trees_Random_Forests_SVMs.ipynb) to go to notebook for week 3 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-WlA6efBRki"
   },
   "source": [
    "##Setup\n",
    "\n",
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zw6fcA3O-Uls"
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "rnd_seed = 42\n",
    "rnd_gen = np.random.default_rng(rnd_seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"dim_reduction\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3QU33M3D--N"
   },
   "source": [
    "## Dimensionality Reduction using PCA\n",
    "\n",
    "This week we'll be looking at how to reduce the dimensionality of a large dataset in order to improve our classifying algorithm's performance! With that in mind, let's being the exercise by loading the MNIST dataset.\n",
    "\n",
    "###**Q1) Load the input features and truth variable into X and y, then split the data into a training and test dataset using scikit's train_test_split method. Use *test_size=0.15*, and remember to set the random state to *rnd_seed!***\n",
    "\n",
    "*Hint 1: The `'data'` and `'target'` keys for mnist will return X and y.*\n",
    "\n",
    "*Hint 2: [Here's the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for train/test split.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mnist dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H9slNfR3D-kg"
   },
   "outputs": [],
   "source": [
    "#Complete the code below\n",
    "# Load X and y\n",
    "X = mnist['data']\n",
    "y = mnist['target'].astype(np.uint8)\n",
    "\n",
    "# Import the train/test split function from sklearn\n",
    "\n",
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.15, \n",
    "                                                    random_state = rnd_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhBQOdVxfr2U"
   },
   "source": [
    "We now once again have a training and testing dataset with which to work with. Let's try training a random forest tree classifier on it. You've had experience with them before, so let's have you import the `RandomForestClassifier` from sklearn and instantiate it.\n",
    "\n",
    "###**Q2) Import the `RandomForestClassifier` model from sklearn. Then, instantiate it with 100 estimators and set the random state to `*rnd_seed!*`**\n",
    "\n",
    "*Hint 1: [Here's the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for `RandomForestClassifier`*\n",
    "\n",
    "*Hint 2: [Here's the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for train/test split.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZZaWwNGUg9Qb"
   },
   "outputs": [],
   "source": [
    "# Complete the code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf =RandomForestClassifier(n_estimators=100, #Number of estimators \n",
    "                               random_state=rnd_seed) #Random State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi1HTS-KjUJ8"
   },
   "source": [
    "We're now going to measure how quickly the algorithm is fitted to the mnist dataset! To do this, we'll have to import the `time` library. With it, we'll be able to get a timestamp immediately before and after we fit the algorithm, and we'll get the time by calculating the difference.\n",
    "\n",
    "###**Q3) Import the time library and calculate how long it takes to fit the `RandomForestClassifier` model.**\n",
    "\n",
    "*Hint 1: [Here's the documentation](https://docs.python.org/3/library/time.html#time.time) to the function used for getting timestamps*\n",
    "\n",
    "*Hint 2: [Here's the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit) for the fitting method used in `RandomForestClassifier`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EZaQPn2XkV06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 45.54s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time() # Load the timestamp before running\n",
    "rnd_clf.fit(X,y) # Fit the model with the training data\n",
    "t1 = time.time()  # Load the timestamp after running\n",
    "\n",
    "train_t_rf = t1-t0\n",
    "\n",
    "print(f\"Training took {train_t_rf:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0-hEhlOnLqh"
   },
   "source": [
    "We care about more than just how long we took to trian the model, however! Let's get an accuracy score for our model.\n",
    "\n",
    "###**Q4) Get an accuracy score for the predictions from the RandomForestClassifier**\n",
    "\n",
    "*Hint 1: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) for the `accuracy_score` metric in sklearn.* \n",
    "\n",
    "*Hint 2: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict) for the predict method in `RandomForestClassifier`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lscBW_sFnLVS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Model Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score # Import the accuracy score metric\n",
    "\n",
    "# Get a set of predictions from the random forest classifier\n",
    "y_pred = rnd_clf.predict(X_test)   # Get a set of predictions from the test set\n",
    "rf_accuracy = accuracy_score(y_test,y_pred)  # Feed in the truth and predictions\n",
    "\n",
    "print(f\"RF Model Accuracy: {rf_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEZX7xBAHJj9"
   },
   "source": [
    "Let's try doing the same with with a logistic regression algorithm to see how it compares. \n",
    "\n",
    "###**Q5) Repeat Q2-4 with a logistic regression algorithm using sklearn's `LogisticRegression` class. Hyperparameters: `multi_class='multinomial'` and `solver='lbfgs'`**\n",
    "\n",
    "*Hint 1: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for the `LogisticRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kwX8ZwzQI6p6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 13.76s\n",
      "Log Model Accuracy: 92.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gjuri\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf =LogisticRegression(multi_class=\"multinomial\", #Multiclass\n",
    "                solver=\"lbfgs\", \n",
    "                random_state=42) #Random State\n",
    "\n",
    "t0 = time.time() # Timestamp before training\n",
    "log_clf.fit(X_train, y_train) # Fit the model with the training data\n",
    "t1 = time.time() # Timestamp after training\n",
    "\n",
    "train_t_log = t1-t0\n",
    "print(f\"Training took {train_t_log:.2f}s\")\n",
    "\n",
    "# Get a set of predictions from the logistric regression classifier\n",
    "y_pred = log_clf.predict(X_test)   # Get a set of predictions from the test set\n",
    "log_accuracy = accuracy_score(y_test, y_pred)  # Feed in the truth and predictions\n",
    "\n",
    "print(f\"Log Model Accuracy: {log_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_5XiaQfJ5NV"
   },
   "source": [
    "Up to now, everything that we've done are things we've done in previous labs - but now we'll get to try out some algorithms useful for reducing dimensionality! Let's use principal component analysis. Here, we'll reduce the space using enough axes to explain over 95% of the variability in the data...\n",
    "\n",
    "###**Q6) Import scikit's implementation of `PCA` and fit it to the training dataset so that 95% of the variability is explained.**\n",
    "\n",
    "*Hint 1: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) for scikit's `PCA` class.*\n",
    "\n",
    "*Hint 2: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform) for scikit's `.fit_transform()` method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEOCAYAAACaQSCZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAsO0lEQVR4nO3deXxcVf3/8dcne5s03RdoKS3QspZCqSwiuyKgX0URRRBRRBREUcS9/ERAkeWrXxFF+KqAgIoLCAqiIIv0qwJla2mhLdB9X7M168zn98e5odPpJLlJZzKT5P18POYxd849c+8naTqfOefce465OyIiIl0pyncAIiLSNyhhiIhILEoYIiISixKGiIjEooQhIiKxKGGIiEgsJfkOIJtGjRrlkyZNyncYIiJ9yvPPP7/R3Ud3VS92wjCzCuC9wN7Are6+1cz2Bra4++aeh5o9kyZNYs6cOfkOQ0SkTzGzZXHqxUoYZrYP8CgwBBgG/B7YClwUvb6gBzGKiEgfEncM438ICWMs0JhS/iBwQpZjEhGRAhS3S+rtwJHunjCz1PLlwO5Zj0pERApOd66SKs1QNhGoyVIsIiJSwOImjL8Dl6W8djOrBr4DPBT3ZGZ2iZnNMbNmM7uji7pfMrO1ZlZjZr80s/K45xERkeyLmzAuA95hZguBCuBeYCkwDvh6N863GrgG+GVnlczs3dFxTwImAXsRkpOIiORJrDEMd19tZocAHwVmEBLNbcA97t7Y2XvTjnMfgJnNBCZ0UvU84BfuPj+qfzVwD91LTiIikkWx78OIEsMv6aJ1kCUHAg+kvH4ZGGtmI919Uy+cX6RgJZNOazJJW8JpSzgtiSRt0euWRHhOJJ2kh8f2bbZvJyHh7dvtddj5PVE9dyeRBMdxBwdwp301HXfw9NeEMnYo87T9O5aR8p7UOrD93LmU08PnOPiZk0Zw7NQu773bJXHvw/gusMLdf5ZW/llgvLtfkeW4qthxML19ewiwQ8IwswuBCwEmTpyY5TBEOpZMOg0tbWxrSVDf3Ma25ui5pY3G1gRNrUma28JzU2uC5tYETW3J8NyapKktQVNavea2JK2J8GhL+I7bySStUTKQvmnHi0yz67PH7V0YCQM4FzgzQ/nzwDeAbCeMeqA65XX7dl16RXe/jdA9xsyZM/U/Sbrk7jS2JqhtbKOmsZXaplZqtkXPja1vldc3t9LQkqAhJRk0tLTR0BzKGlsT3T53RWkRFaXFlJeE54qSYipKiygvKWZIRQmjSoopKzFKi4soKSqirMQoKSqipNgoKw7PobyIkqJQr7TYKCku2r5dVERxERSZUVxkFJlRVGQUm1FkUBSVtdfZsR4Um2FRWdhm+/72DzwDw976ADTAzKLn9jIL9TLUIaqXeoxMZRnfk8tPXelU3IQxBtiQoXwT4Wa+bJsPTAd+F72eDqxTd5SkSyadmsZWNjW0sLmhhc0NzWG7vuWtsq2NIRHURc+1Ta20Jjr/blFVXkJVeQmV5cVUlpdQWVbC7sMqwnZ5CZVl28sr2+uVlTA4eh5UlpIMogRRXlKkDzvp0+ImjOXAMcCbaeXHAivjnszMSqJzFgPF0fxUbe7ellb1V8AdZnYPsAaYBdwR9zzSt7k79c1trK9rZl1tE+trw/O62mbW1zWxsb45Sg4tbNnW2mEXzZDyEkZUlTFscBlDB5Wyx/BBDB1USvWg0vBcET0PKtnh9ZCKEkqKNZGzSLq4CeNW4IdmVgY8HpWdBFwLXNeN880Cvp3y+mPAd8zsl8AC4AB3X+7uj5jZ9cATwCDgj2nvkz7K3altbGPl1m2s2tLIqq2NrN7ayNraZtbXNr2VJLa17NzdM6i0mLHV5YweUs7kUZUctucIRlaWMaKyjJFV4XlEZRkjK8sZXllKeUlxHn5Ckf7LPObIvZldC3wRKIuKWoAfuXvBXOo6c+ZM12y1+dfUmmD55m0s2djAsk0NrNgcEkN7gqhv3rFBWVFaxLjqCsZUVzBmSDljqysYW13OmCEVjKkOr8cMKaeqvERdOiI5YGbPu/vMrup157Lab5jZNcABhLGoBe5evwsxSh+WSDrLN2/j9fX1LN3YwJJNITks3biN1TWNO1xBWF1Rwvjhg9ljxGCO2nsk44cNYvzwQYwfNogJwwcxorJMiUCkD+jWAkru3gA8l6NYpAC5O2tqmli4ro5Fa+vC87o6Fq+rp7kt+Va9YYNLmTSykrdNGs6kUROYPKqSSSPDY+jgTNOQiUhfE/c+jArgUsK4xRjSphRx94OzH5r0NndnxeZG5q7ayryVNcxdWcMrq2uoa9rehTS2upypY4dw7pF7MnXcEKaMqWLyqEqGDS7r5Mgi0h/EbWH8FPgAYeGkf5HjGyKldzQ0t/Hi8q08u2QTL67YytyVNdQ0tgJQVlzE/rsN4X3Td2e/3arZd+wQpo6tUmIQGcDiJozTgTPd/bEcxiI5Vt/cxn/e2MQzSzbx7JLNvLK6lkTSKTLYb1w1p00bx7Txwzh4wlCmjh1CWYkuLRWR7eImjG3AilwGItnn7ixYU8s/F23kqUXreX7ZFloTTllxEYfsMYzPHrcXh08eyYyJwxhSoXEGEelc3IRxPXCZmV3k7skua0veJJLOc0s38/C8NTzyylrW1zUDsP9u1Zz/jskcN2U0M/YcTkWp7lEQke6JmzDeRbjT+xQzWwC0pu509/dlOzCJz915ccVW7nthJY+8so6N9c1UlBZxwr5jOGn/sRw7ZRRjqivyHaaI9HFxE8ZG4P5cBiLdt6m+mftfXMW9z61g8fp6KkqLOHG/MZw2bTdO2HcMleXdumpaRKRTcRdQ+mSuA5H4Xl6xlZ/PXsIjr6yhNeEcOnEY3//gNN47fXeqlCREJEf06dJHJJPOo6+u4+dPv8lzS7cwpLyEc4+cxFmH78HUsUPyHZ6IDACxE4aZfZKwROtEts8nBYC775XluCTi7jy5aAM3/m0h81fXMmH4IK547wF85G17qDUhIr0q7p3eXyEslHQrYUrznwL7RNs35iy6AW7eyhqu+st8nlu6hYkjBvODD0/nfdN319TbIpIXcb+ifhq40N3/YGaXADe7+5tmdgWwZ+7CG5hqtrVy498XcvczyxhZWc41px/Eh2fuoRvpRCSv4iaMCcCz0XYj25dM/U1U/uksxzVg/XXeGmb96RW2bGvhvKMmcdnJU6nWTXUiUgDiJoy1wCjCynvLgKOAlwjdUppXKgvqmlq58sEF/PGFlUwbP5Q7zz+cg8YPzXdYIiJviZswHgfeB7wA/IKw+t6HgRlsX3dbemjh2jo+c9cclm/exhdO3IfPnzSFUo1TiEiBiZswLiSa0tzdf2ZmW4CjCUun3pqj2AaEB19ezdf+MJeqihJ+e+FRHD55RL5DEhHJKO6Ne0kgmfL6XuDeXAU1ELg7P3xsMTf9YzFvmzScn5w9Q9N3iEhB6zBhmNkM4CV3T0bbHXL3F7IeWT+WTDpX/WUBd/xrKWceNoHvfXCauqBEpOB11sKYA4wD1kfbTljLO50Dmvo0pmTS+dof5/L751fy6WMm883T9td61iLSJ3SWMCYDG1K2ZRe5O1c/tIDfP7+SL5w0hS+9c4qShYj0GR0mDHdfBmBmpcDngJ+0l0nP/Pjx17n9/5Zy/tGTlSxEpM/psuPc3VuBi8ncHSUxPfDSKn7w6CLOmDGBWe9RN5SI9D1xR1r/BpyYy0D6s/mra/jaH+dy+KQRfP+MaRQVKVmISN8T9z6MfwDfM7ODgeeBhtSd7n5ftgPrL2qbWvnMXc8zbFAZPzlnhq6GEpE+K27CuDl6/kKGfbpKqhNXPjCfNTVN/O4zRzF6SHm+wxER6bG4N+7pa3EPPDR3Dfe9uIpLT5rCYXsOz3c4IiK7RIkgR9bXNvGtP81j+oShXHLiPvkOR0Rkl3Vnxb0RwClkXnHvqizH1edd/dCrbGtJ8IOPHKJxCxHpF+KuuHck8BDQDIwGVgG7Ra+XAkoYKf79xib+/PJqvnDSFPYeXZXvcEREsiLuV98bgHuA8UAT4RLbiYQpQ67LTWh9U2siyZUPzmfC8EFcfPze+Q5HRCRr4iaMgwnLsjqQAMrdfR3wNeDKHMXWJ9373AoWrqtj1nsOoKJUF4+JSP8RN2G0pGyvY/s63vXA7lmNqA9rak3w48cXc9iew3n3gWPzHY6ISFbFTRgvAG+Ltp8ErjGz84CbgLlxT2ZmI8zsfjNrMLNlZnZ2B/XMzK4xs1VmVmNmT5rZgXHPky93/2cZ62qbufzkfTX1h4j0O3ETxreA1dH2LMIstj8GhhNW44vrJ4TWyljgHOCWDhLBmcD5wDHACODfwF3dOE+va2pN8LOn3uTofUZy1N4j8x2OiEjWxb1xb07K9gbg1O6eyMwqgTOAg9y9HphtZg8C5wJfT6s+GZjt7m9G770b+FJ3z9mbHnhpFRvrm/nR8YfkOxQRkZyI1cIwsx92tepeDFOBhLsvSil7GcjUwvgtsI+ZTY2mVz8PeGQXz58z7s7Pn17CAbtV83a1LkSkn4rbJXUEMMfMXjWzb5rZpB6cqwqoSSurAYZkqLsGeBpYCDQSuqgytjDM7EIzm2NmczZs2JCpSs49tWgDi9fX8+ljJ2vsQkT6rVgJw93fDuxNuBfjY8AbZva0mX3GzOJOklQPVKeVVQN1Gep+mzDIvgdQAXwHeNzMBmeI7TZ3n+nuM0ePHh0zlOz67bMrGFVVxnum6YIxEem/Ys9Z4e5L3P0adz+A8GH+DHAF2wfDu7IIKDGzKSll04H5GepOB+5195Xu3ubudxAG2A+IG29v2VTfzD9eW8cHDh1PWYmmABGR/qunn3ClQDlhTqlEnDe4ewNwH3CVmVWa2dHA+8l89dNzwJlmNtbMiszs3Oicr/cw3pz500uraU04Z87cI9+hiIjkVOyEEQ1Af8fMFgOzgX2BywmXyMZ1MTAIWA/8BrjI3eeb2UQzqzeziVG96wgD4i8BWwnjF2e4+9ZunCvn3J3fz1nB9D2GMXVspqEYEZH+I+7kg3OAQwkf4rcAv3b3td09mbtvBk7PUL6cMCje/roJ+Fz0KFivrKrltbV1XHP6QfkORUQk5+JOb/534Fx3fzWXwfQ1f567mtJi478O1mC3iPR/cW/c+2auA+lr3J2H563hHfuMYujg0nyHIyKSc7qsp4deWVXLyi2NnDptt3yHIiLSK5QweuiR+WsoLjJOPkCz0orIwKCE0UNPvLaBw/YczrDBZV1XFhHpB5QwemB9bRML1tRy/L75ubNcRCQfOhz0TrknokvRZbEDxpOLwpxVx08dk+dIRER6T2dXSS0FPOZxBtRapE8t3MCYIeXsv5tu1hORgaOzhPG2lO2pwPXAzwiLGQEcBXyGsK73gNGWSPL04g28+8BxmplWRAaUDhOGuz/fvm1mPwC+5O5/SKnyuJktBC4lTPMxICxYU0ttUxvvmDIq36GIiPSquIPeh5N57e65wGHZC6fwPbtkMwBHTNZCSSIysMRNGEsJEwemuxhYlrVo+oBnl2xm4ojBjBtake9QRER6Vdy5pL4E3G9mpwD/icqOACYBH8xBXAXJ3Xlu6WZO3E8364nIwBN3xb1HgCmE9SyqgaHR9lR3/2vuwissb2yoZ8u2Vo6YPCLfoYiI9Lq4LQzcfSUwoCchfHH5VgBm7Dksr3GIiORDdxZQmmZmN5vZw2a2W1R2upkdmrvwCsvclTVUlZew16iqriuLiPQzsRKGmZ1MWDZ1PHASYdU8gL2Bb+cmtMIzd+VWDhpfTVGR7r8QkYEnbgvjauAyd/8A0JJS/iThktt+r6Utyatr6pg+YVi+QxERyYu4CeNA4OEM5ZuBATEC/NraWloSSQ5WwhCRASpuwthC6I5KNwNYmb1wCtfclTUAHDxhaJ4jERHJj7gJ49fADWY2gTAhYYmZHQfcCPwqV8EVkrkrtzKisowJwwd1XVlEpB+KmzBmAUsId3VXAQuAx4HZwHdzE1phmbeqloPGD9WEgyIyYMW6D8PdW4FzzOz/AYcSEs2L7r44l8EVitZEkjfW13PsVE04KCIDV+wb9wDc/Q3gjRzFUrCWbmygJZFkv3Fa/0JEBq7YCcPMPkK4B2MMaV1Z7v6+LMdVUF5bWwfAvmOr8xyJiEj+xEoYZnYD8EXgCWA18Vfi6xcWrq2juMjYe0xlvkMREcmbuC2MjwMfTVtAacB4bW0de42qpLxkQK1EKyKyg7hXSRUBL+UwjoK2cF0t+2r8QkQGuLgJ4zbgY7kMpFA1tSZYsbmRKWOUMERkYIvbJTUMONvM3kVYlrU1dae7fyHLcRWM5Zu3ATBp1OA8RyIikl9xE8YBbO+S2i9tX78eAF+ysQGAyaM04C0iA1vcG/dOyHUghWpplDD2HKmEISIDW+wFlAaqpZsaGFFZxtBBpfkORUQkrzpsYZjZg8DH3L022u5Qf75xb8nGBiaN1PiFiEhnLYxNbB+f2NTFIxYzG2Fm95tZg5ktM7OzO6m7l5n9xczqzGyjmV0f9zzZtGzTNiZp/EJEpOMWhrt/MtP2LvoJYcW+scAhwENm9rK7z0+tZGZlwKNR/Y8ACWBqlmKIrbElwZqaJiZr/EJEpPfGMMysEjgDuMLd6919NvAgcG6G6p8AVrv7D9y9wd2b3H1ub8XabtnmaMBbLQwRkW5NPngC8FFgIlCWus/dT4xxiKlAwt0XpZS9DByXoe6RwFIz+yvwNuAV4PPuPi9uvNnQfoWUWhgiIjFbGGb2CeCvwBDgeGADMJywROuCmOeqAmrSymqiY6abAJwF3ATsDjwEPBB1VaXHdqGZzTGzORs2bIgZSjxLN4Wb9vbUTXsiIrG7pC4HLnH3jxLu8v6Gux8K3A3UxzxGPZA+P3g1UJehbiMw293/6u4thKVgRwL7p1d099vcfaa7zxw9enTMUOJZtaWRoYNKqa7QJbUiInETxl7AY9F2M6G1AHAzYbwhjkWEtcCnpJRNB+ZnqDuXAriDfE1NI7sNrch3GCIiBSFuwtjE9q6jVcBB0fZIYFCcA7h7A3AfcJWZVZrZ0cD7gbsyVL8bONLM3mlmxYS1ODYCr8aMNytWb21i92GxfjwRkX4vbsJ4Gjg52v4dcJOZ3Q78hnD5a1wXExLM+ui9F7n7fDObaGb1ZjYRwN0XEmbH/RmwhZBY3hd1T/WatbVNjFMLQ0QEiH+V1CVA+yfntUAbcDQheVwT92Tuvhk4PUP5crZ3c7WX3UdokeRFU2uCzQ0t7K6EISICxJ98cHPKdhK4LmcRFYg1NU0AjBuqLikREeh8LqkRcQ+SmlD6izU1jQBqYYiIRDprYWyk6yuVLKrT7xa7XrM1tDB206C3iAjQecIYsGtgQBjwBhhXrRaGiAh0PvngU70ZSKFZvbWR4YNLGVTW7xpPIiI90p25pCqAswnLtUKYEuQ37t6Yi8DybV1tE2PVuhAReUvcuaRmAG8C/w0cHj1uBN6M9vU7G+pbGD2kPN9hiIgUjLg37t0GzAYmuPux7n4ssAfwz2hfv7OxrpnRVUoYIiLt4nZJHQh8PJreAwhTfZjZVcCcnESWR+7OhvpmtTBERFLEbWG8RphmPN1uhEkF+5W65jZa2pKMUgtDROQtcVsYswjzR10F/CcqOzIq/3rqTX794Sa+jXXNAIwastPyGyIiA1bchPHn6PnXbL+Zz6LnB1Je94ub+Da0Jwy1MERE3hI3YQyom/g21odJcTWGISKyXdzJBwfUTXwb69XCEBFJF/c+jJvMbKdPTzMbY2YPZT+s/NpY30yRwfDBGsMQEWkX9yqpU4E5Zta+0h5m9l/AK8Rcca8v2VDXzIjKcoqLrOvKIiIDRNyEcQjwHPCcmV1mZrcAfwB+CJyUo9jyZqPuwRAR2UncMYwG4HwzW0mYEqQNeFd/HdvYUN/CqCp1R4mIpIrbwsDMvgx8BbgDWAjcZmYzcxRXXm1uaGZkpRKGiEiquIPejwJfA85y9/OBmcDfgdlm9q0cxpcXNdtaGTqoNN9hiIgUlLgtDAemu/sDAO7e7O6fB04HLslRbHmRTDp1zW1KGCIiaeKOYZzcQfkjZjYtuyHlV11TG+5QrYQhIrKD7oxhTDOzm83sr2a2W1R2OmGa836jprEVQC0MEZE0cccwTiZcVjseOJHt917sDXw7N6HlhxKGiEhmcVsYVwOXufsHgJaU8icJq+/1G0oYIiKZxU0YBwIPZyjfDIzIUN5nvZUwBithiIikipswthC6o9LNAFZmL5z829oYGlBqYYiI7Chuwvg1cIOZTSBcYltiZscR7vr+Va6Cywd1SYmIZBY3YcwClgDLgCpgAfA4MBv4bm5Cy4+axlbKiosYVNrn14ESEcmquPdhtALnmNkVhG6oIuBFd1+cy+DyobaxlepBpZhpploRkVRxV9wDwN3fBN7MUSwFoaaxlaGDuvVrEREZEGLfuDdQhISh8QsRkXRKGGmUMEREMlPCSKOEISKSWa8mDDMbYWb3m1mDmS0zs7NjvOdxM3Mz65WBBU1tLiKSWXcmHxxrZpeb2S1mNioqO9rMJnfjfD8hTC0yFjgHuMXMDuzknOfQzYH5XdE+tblmqhUR2VncyQcPI6yydw7wKaA62vUuYt6HYWaVwBnAFe5e7+6zgQeBczuoP5QwseFX4xw/GxpbE7hDVbmukhIRSRe3hXEj8CN3PxRoTin/G3B0zGNMBRLuviil7GXCPFWZfA+4BVgb8/i7rKGlDYDBShgiIjuJmzAOA+7MUL6G0L0URxVQk1ZWAwxJrxitFX408OOuDmpmF5rZHDObs2HDhpihZLatOQFAZZnu8hYRSRc3YTQCwzOU7wesj3mMerZ3ZbWrBupSC8ysCPgpcKm7t3V1UHe/zd1nuvvM0aNHxwwls7daGGVqYYiIpIubMB4Avm1m5dFrN7NJwHXAH2MeYxFh0sIpKWXTgflp9aqBmcC9ZraWsHATwEozOybmuXpkW0vUwihXC0NEJF3chHE5Yd2LDcBgwqSDrwNbCRMTdsndG4D7gKvMrNLMjgbeD9yVVrUG2B04JHqcFpUfBjwTM94eaWhWC0NEpCNxJx+sBd5hZieyffLBF9z9sW6e72Lgl4RurE3ARe4+38wmEmbAPcDdl5My0G1mFdHmujhdVLtCLQwRkY7FShhmNt3dX3b3xwnTmveIu28GTs9QvpwwKJ7pPUuBXpk6tr2FUakWhojITuJ2Sb1oZvPM7KvRIkr9UnsLY7CukhIR2UnchLEfYfzhAmCpmT1hZuebWfpVT31a+1VSlboPQ0RkJ7EShrsvcvdvu/tUwv0R8wg31q01s9/lMsDetK05QZFBeYnmZBQRSdftT0Z3f8bdv0C4wmkhYbqPfqGhpY3KshKtticikkG3EoaZ7WVms8zsVcKltVsI3VT9wrbmBIN1hZSISEZxr5L6HGHiwSOAV4DbgXvcfVUOY+t17S0MERHZWdxPx68DvwE+4+7zchhPXm1rUQtDRKQjcRPGRHf3nEZSABqa23SXt4hIBzr8dDSzGcBL7p4EDu1sINjdX8hBbL2usTXB8MFl+Q5DRKQgdfZ1eg4wjjCNxxzAyXzHtQP9oh+nqTVBRakuqRURyaSzhDGZMNlg+3a/19SapKK0X+Q+EZGs6zBhuPuy1JfAikzjGNHEgf1Cc1uCihIlDBGRTOL2vywBdlqdyMxGRvv6hdDCUJeUiEgmcT8djdDKSFcFNGUvnPwKYxhqYYiIZNLpNaRmdlO06cC1ZrYtZXcxcDjwUm5C613uTnNbknIlDBGRjLq66WBa9GzA/kBLyr4W4AXgxhzE1eua25KAJh4UEelIpwnD3U8AMLPbgUujlff6pebWkDDUJSUiklncJVo/metA8q2pLSyepEFvEZHMYs+DYWYnAB8FJgI73A7t7idmOa5e19QaJQxdVisiklGsr9Nm9gngr8AQ4HjCDX3DgRnAghzF1qua1CUlItKpuP0vlwOXuPtHgVbgG+5+KHA3UJ+r4HpTewtDg94iIpnF/XTcC3gs2m4m3H8BcDPwiSzHlBdvdUmphSEiklHchLGJ0B0FsAo4KNoeCQzKdlD50H5ZrQa9RUQyizvo/TRwMjAP+B1wk5m9CzgJeDRHsfUqtTBERDoXN2FcAlRE29cCbcDRhORxTQ7i6nVNamGIiHQq7n0Ym1O2k8B1OYsoT7YPequFISKSSWcr7o2Ie5DUhNJXNbcnDLUwREQy6qyFsZHMM9Smap/Fts9/Ld8+6N3nfxQRkZzoLGGc0GtRFADd6S0i0rnOVtx7qjcDybem1iRFBqXFmZYtFxGRWIPeXY1n9IcxjKbWBOUlxZgpYYiIZBL3stquxjP6fD9OSyKpAW8RkU7ETRjp4xmlwKHARcCsrEaUJy1tScqKlTBERDoS9z6MTOMZj5nZm8AFwK+zGlUetLQlKdPEgyIiHdrVT8iXgGPjVjazEWZ2v5k1mNkyMzu7g3rnmdnzZlZrZivN7Hozi712R080J5QwREQ60+NPSDOrAr4IrOjG235CWAt8LHAOcIuZHZih3uDo2KOAIwhzVl3e01jjaFWXlIhIp+JeJVXHjoPeRvhQbyB88Mc5RiVwBnCQu9cDs83sQeBc4Oupdd39lpSXq8zsHnJ8X0iLWhgiIp3qzuSDqZKEVfeecfctMY8xFUi4+6KUspeB42K891hgfqYdZnYhcCHAxIkTY4ayMw16i4h0Lu6g951ZOFcVUJNWVsP2dTYyMrNPAjMJg+uZYrsNuA1g5syZXU1l0iENeouIdK5bA8nRDXxjSBv7cPc463rXA9VpZdVAXSfnOx34PvBOd9/YnVi7qyWRpKoip+PqIiJ9WtwxjEOB24Fp7UWEMY3uTD64CCgxsynuvjgqm07HXU2nAP8LvMfd58WJc1e0tCUpVZeUiEiH4n6l/iVhadZLgXV0PYvtTty9wczuA64yswuAQ4D3A29Pr2tmJwL3AB9w92e7e66e0KC3iEjn4iaMKcCZ7v76Lp7vYkLyWU9YJ/wid59vZhOBBcAB7r4cuAIYCjycMrfT0+5+6i6ev0MtbUnK1cIQEelQ3IQxG9gf2KWEEU1SeHqG8uWEQfH2170+tboGvUVEOhc3YXwK+LmZ7QW8ArSm7nT3f2Y7sN6mLikRkc51p0vqEODdGfb1ixX3dB+GiEjn4iaMW4F/ANfSw0HvQteaSFKqFoaISIfiJowJwGnu/kYug8mXZNJpTbhaGCIinYj7CfkocFguA8mnlkQSQGMYIiKdiNvCeAT4bzM7GJjHzoPe92U7sN7UnjDKlTBERDoUN2H8NHr+ZoZ9fX7Qu6VNLQwRka7EnXywX3+SticMTQ0iItIxfUISrpACNOgtItKJuJMPXtbZfnf/QXbCyQ91SYmIdC3uGMbn016XArsBjYR5ofp0wmhWwhAR6VLcMYzJ6WVmNpYw5fn/Zjuo3qbLakVEutbjT0h3Xwd8C7g+e+HkR3uXlGarFRHp2K5+QhYBY7MRSD61D3prahARkY7FHfT+YHoRYQzjc8DT2Q6qt7016K0WhohIh+IOev8h7bUDG4DHgS9nNaI80FVSIiJd0417wJjqck6bNo5hg0vzHYqISMGK28Lo1w7bcwSH7Tki32GIiBS0TlsOZnaqmS01s6EZ9g2N9p2cu/BERKRQdNXVdAlwg7vXpO+Iyq4DLs1FYCIiUli6ShgHA491sv9xYHr2whERkULVVcIYDSQ72e/AyOyFIyIihaqrhLGS0MroyMHAquyFIyIihaqrhPEQcLWZDUrfYWaDgauiOiIi0s91dVntd4EPAYvN7MfAa1H5/oQBcQO+l7vwRESkUHSaMNx9vZm9HbiFkBisfRfwN+DiaBJCERHp58zd41U0Gw7sQ0gai919Sy4D6wkz2wAs6+HbRwEbsxhONim2nlFsPaPYeqZQY4sT157uPrqrA8VOGP2dmc1x95n5jiMTxdYziq1nFFvPFGps2YyrX88RJSIi2aOEISIisShhbHdbvgPohGLrGcXWM4qtZwo1tqzFpTEMERGJRS0MERGJRQlDRERiGfAJw8xGmNn9ZtZgZsvM7OxePPclZjbHzJrN7I60fSeZ2Wtmts3MnjCzPVP2mZldZ2abosf1ZmY7nWDXYis3s19Ev5M6M3vRzE4thPjM7G4zW2NmtWa2yMwuKIS40mKcYmZNZnZ3ocRmZk9GMdVHj4WFElt0nrPM7NXo/+IbZnZMvmNL+V21PxIWZr1o35/vf9NJZvawmW0xs7VmdrOZleQsNncf0A/gN8C9QBXwDqAGOLCXzv1B4HTCnfR3pJSPiuI4E6gAbgD+k7L/M8BCYAIwHlgAfDbLsVUCVwKTCF8s3gvURa/zGh9wIFAebe8HrAUOy3dcaTH+HXgauLuA/k2fBC7IUF4Isb2LcNPtkdHf2/jokffY0v5P1APHFtDv7WHgjuj844B5wBdyFVvWf6l96RH9AbQAU1PK7gK+38txXMOOCeNC4F9pcTYC+0Wv/wVcmLL/U6l/DDmMcy5wRiHFB+wLrAE+XChxAWcBvyMk3PaEkffY6DhhFEJs/wI+VYixpRz7POBNtl8slPfYgFeB01Je3wDcmqvYBnqX1FQg4e6LUspeJnyDzacDozgAcPcG4A22x7XDfnohZjMbS/h9zS+E+Mzsp2a2jTAh5hrCN61CiKuaMIvzl9N25T22yLVmttHM/s/Mji+E2MysGJgJjDaz181sZdS1MijfsaU5D/iVR5+wBRLbj4CzzGywmY0HTgUeyVVsAz1hVBGabalqgCF5iCVVV3Gl768BqnLYH18K3APc6e6vFUJ87n5xdL5jgPuA5kKIC7ga+IW7r0grL4TYvgbsReiCuA34s5ntXQCxjQVKCTNjHwMcAhwKzCqA2AAws4nAccCdKcWFENtThA/6WsL6RXOAP+UqtoGeMOqB6rSyakJffT51FVf6/mqgPuWbT9aYWRGhm66FMKV9wcTn7gl3n03oh70o33GZ2SHAO4EfZtid99+Zuz/j7nXu3uzudwL/B5xWALE1Rs8/dvc17r4R+EGBxNbu48Bsd1+SUpbvv7ciwqzh9xG6nEYBw4HrchXbQE8Yi4ASM5uSUjad0O2ST/NJWSvdzCqBvdke1w77yVHM0beNXxC+AZ7h7q2FFF+KkpTz5zOu4wkXBSw3s7XA5cAZZvZCAcSWiRNmn85rbB5mvl4ZxZOuUH5vH2fH1kUhxDYC2AO4OfoSsAm4nZBocxNbLgaH+tID+C3hSqlK4Gh69yqpEsIVDNcSvsVXRGWjozjOiMquY8crHD5LGOwaD+we/UNn/coQ4GfAf4CqtPK8xQeMIQwqVwHFwLuBBuD9+f69AYMJV6q0P24E/hDFle/YhkW/q/a/sXOi39u++Y4tOsdVwHPRv+9wwhVmVxdIbG+PfldDCuX/Qco53gS+Hv2bDgPuJ3Qf5yS2rAXeVx+ELP2n6A9iOXB2L577SsK3qtTHldG+dxIGdBsJV7dMSnmfAdcDm6PH9URXbmQxtj2jeJoIzdf2xzn5jC/6j/AUsJXQbzsP+HTK/rz+3jL8+95dCLFFv7fnCF0SWwlfBN5VCLFF5ygFfhrFtha4CagokNhuBe7qYF++YzskOu8WwpoXvwfG5Co2zSUlIiKxDPQxDBERiUkJQ0REYlHCEBGRWJQwREQkFiUMERGJRQlDRERiUcIQEZFYlDCkoJjZHWbmZjYrrfz4qHxUStl7ogV33jSz82Ice4iZXW1mC8ys0czWWVhU6KPRvDwSiX7XH8p3HFJYSvIdgEgGTcBXzexWd9+QqYKZlROmLrmAcJf+nWb2uO88S2x7/WHAbMK0E7OAZwkTKr4DuAL4N7A0uz+GSP+ib1VSiJ4gfHhf0UmdMiABvEhY2GkrnU9L/z1gMnCEu9/u7vPdfbG73w7MIExHgZkNN7M7oyUvG83sMTN7a50AM/tEtFTnqSnLXz5oZkPN7ENmttjMaszsrmg9h/b3PWlmPzOzH0XH3mJmN6S2bLpx7pPM7BULS5k+YWaTU39QM/svM3vewnKsS8zsu2ZWlrJ/qZnNMrNbLSxzu9LMvpK6P9r8fdTSWBqV72FmD5jZ5ujnfs3Mzurkdy79jBKGFKIkYUK1z0brNezE3esIc/ysIsyF8093X5CpbvShfBZwj7uvzHCsJndvil7eARxBmMzwcGAb8Ejqhz9QTlgg6RzgJMLiP38gLLBzBmHZ3fcCF6ed6hzC/7mjCEtkXgh8MWV/3HN/Azg/Os4wQkur/Wd9N2HyuZsJ6yScT1hn4ntpsXyJMA/XDMLEdNeb2VHRvrdFz58Gdkt5/VPCBIsnRMf+IiFRy0CRzYmw9NBjVx+ED82/RNtPAL+Nto8nTIY4Kq3+EGBYF8ccE733S13UmxLVOzalbChh1s8LotefiOrsm1LnRkJrZ1SmnyN6/SRhOn1LKZsFrNzFc59D6Foril7/E7gi7ec6nTBxZPvccUuB36TVWQzMSnntwIfS6swFvp3vvxE98vdQC0MK2VeBM81sZkcVPCwItLWL48Rd4Wx/Quvm3ynHryF8Ez8gpV6zuy9Meb0OWOth4Z/UsjFpx/+Pu6fO9vlvYLyFZV17eu7VhJleh0WvDwO+FXVd1ZtZPfBrwvT941LeNzctttUZ4k33I2CWmf3bzK4xs8O6qC/9jBKGFCx3fw74I6HLZFdsIEz/vH8X9TpLLKkf9G0Z9rVmKOvO/69dOTcp5yoCvkOY9rr9cTChBZN6AUG343X3XxDGgW4nrO/+LzO7srP3SP+ihCGF7puEdZ5P6ekB3D0J3AucY2YT0vebWYWZVQAL2D7G0L6vGpgW7dtVR6StmXwksNrda7N47heA/dz99QyP9GTTmVbCAlU7cPeV7n6bu38Y+H+EcRgZIJQwpKC5++vAbcClu3iobxIWyHrGzD5pZgea2T5mdi7wPDDO3RcDDwC3mtkxZjYNuJuwUNOvd/H8EFY2+x8z2ze6x+ErROt/Z/HcVwFnm9lVZnaQme0XXb11fTdjXQqcZGbjzGw4QHSF1ylmtpeF9ctPITuJVPoIJQzpC65i566YbvGwbvSRhMHorxGSxL+ATxGWAl0eVf0k4R6NB6PnwcAp7t64K+eP3EP41v4M8L+E9dJ/mLJ/l8/t7n8D3kO4kunZ6PF1tv98cX05OsYKwqXLED4vfkxIEo8Sxmm6vGFS+g+tuCfSC8zsSeAVd78k37GI9JRaGCIiEosShoiIxKIuKRERiUUtDBERiUUJQ0REYlHCEBGRWJQwREQkFiUMERGJRQlDRERi+f/w5L2VTqhhBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca_pre = PCA(n_components =  None).fit(X_train)\n",
    "cum_pca = np.cumsum(pca_pre.explained_variance_ratio_)\n",
    "plt.plot(cum_pca)\n",
    "plt.xlabel('N° Components')\n",
    "plt.ylabel('Cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rrP5043rJc-1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The explained variability is equal to 94.96%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA # Importing PCA\n",
    "\n",
    "n_components, = np.where(cum_pca >= 0.95)\n",
    "pca = PCA(n_components=n_components[0]) # Set number of components to explain 95% of variability\n",
    "\n",
    "X_train_reduced = pca.fit_transform(X_train) # Fit-transform the training data    -> learn and transform \n",
    "X_test_reduced = pca.transform(X_test) # Transform the test data (!!No fitting!!) -> transform base on what you already learn\n",
    "\n",
    "ev = sum(pca.explained_variance_ratio_)*100\n",
    "\n",
    "print(f'The explained variability is equal to {ev:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKXeXWn4M8K1"
   },
   "source": [
    "###**Q7) Repeat Q3 & Q4 using the reduced X_train dataset instead of X_train.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "m1oZFFfljH0N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 79.79s\n"
     ]
    }
   ],
   "source": [
    "# Complete the code\n",
    "\n",
    "t0 = time.time() # Timestamp before training\n",
    "rnd_clf.fit(X_train_reduced, y_train) # Fit the model with the training data\n",
    "t1 = time.time() # Timestamp after training\n",
    "\n",
    "train_t_rf = t1-t0\n",
    "\n",
    "print(f\"Training took {train_t_rf:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jNisAXlgnUMe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Model Accuracy on reduced dataset: 94.65%\n"
     ]
    }
   ],
   "source": [
    "# Get a set of predictions from the random forest classifier\n",
    "y_pred = rnd_clf.predict(X_test_reduced)   # Get predictions from the reduced test set\n",
    "red_rf_accuracy = accuracy_score(y_pred,y_test)  # Feed in the truth and predictions\n",
    "\n",
    "print(f\"RF Model Accuracy on reduced dataset: {red_rf_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46j-guE8NStk"
   },
   "source": [
    "###**Q8) Repeat Q5 using the reduced X_train dataset instead of X_train.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JerFiDoKMpAx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 6.55s\n",
      "Log Model Accuracy on reduced training data: 91.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gjuri\\.conda\\envs\\ada\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Complete the code\n",
    "\n",
    "t0 = time.time() # Timestamp before training\n",
    "log_clf.fit(X_train_reduced, y_train) # Fit the model with the reduced training data\n",
    "t1 = time.time() # Timestamp after training\n",
    "\n",
    "train_t_log = t1-t0\n",
    "print(f\"Training took {train_t_log:.2f}s\")\n",
    "\n",
    "# Get a set of predictions from the logistric regression classifier\n",
    "y_pred = log_clf.predict(X_test_reduced)   # Get a set of predictions from the test set\n",
    "log_accuracy = accuracy_score(y_test, y_pred)  # Feed in the truth and predictions\n",
    "\n",
    "print(f\"Log Model Accuracy on reduced training data: {log_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P_-tnZstz99"
   },
   "source": [
    "You can now compare how well the random forest classifier and logistic regression classifier performed on both the full dataset and the reduced dataset. What were you able to observe? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AFlS89UuZTy"
   },
   "source": [
    "Write your comments on the performance of the algorithms in this box, if you'd like 😀\n",
    "(Double click to activate editing mode)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3ZLHinjoEVZg3ci9tgSMS",
   "collapsed_sections": [
    "0-WlA6efBRki"
   ],
   "include_colab_link": true,
   "name": "S4_1_Principal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
